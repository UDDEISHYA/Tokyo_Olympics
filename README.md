# Tokyo Olympics 2020 Data Engineering Project
<p align="center">
  <a href="" rel="noopener">
 <img width=800px height=600px src="https://media.telanganatoday.com/wp-content/uploads/2021/07/All-you-need-to-know-about-Tokyo-Olympics-2020.jpg" alt="Project logo"></a>
</p>


<div align="center">

![Azure](https://img.shields.io/badge/Azure-0078D4?style=for-the-badge&logo=microsoft-azure&logoColor=white)
![Databricks](https://img.shields.io/badge/Databricks-FF3621?style=for-the-badge&logo=databricks&logoColor=white)
![Apache Spark](https://img.shields.io/badge/Apache_Spark-E25A1C?style=for-the-badge&logo=apache-spark&logoColor=white)
![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)

**An end-to-end cloud-based data engineering pipeline built on Microsoft Azure**

</div>

---

## üìã Table of Contents

- [Problem Statement](#-problem-statement)
- [Idea / Solution](#-idea--solution)
- [Architecture Overview](#-architecture-overview)
- [Technology Stack](#-technology-stack)
- [Setting up the Environment](#-setting-up-the-environment)
- [Implementation Guide](#-implementation-guide)
- [Usage](#-usage)
- [Data Schema](#-data-schema)
- [Dependencies / Limitations](#-dependencies--limitations)

---

## üéØ Problem Statement

Organizations often struggle with processing and analyzing large-scale event data stored across distributed sources. The Tokyo Olympics 2021 dataset, containing information about 11,000+ athletes, 47 disciplines, and 743 teams, presents several challenges:

- **Data Accessibility**: Raw data scattered across multiple CSV files in external repositories
- **Scalability**: Need for cloud-based infrastructure to handle large-scale data processing
- **Transformation Complexity**: Requirement for robust ETL pipelines to clean and transform raw data
- **Analytics Readiness**: Data must be structured and optimized for analytical queries and visualization
- **Integration**: Seamless connection between ingestion, transformation, and analytics layers

---

## üí° Solution

This project implements a **comprehensive Azure-based data engineering solution** that addresses the challenges through a modern cloud-native architecture:

### Core Solution Components

<a href="" rel="noopener">
<img width="990" height="480" alt="core_solution_components" src="https://github.com/user-attachments/assets/a1f404ca-9424-4842-923e-b04a7d87d92a" />

---

## üèóÔ∏è Architecture Overview
<img width="990" height="480" alt="core_solution_components" src="assets/tokyo_olympics_data_architecture.png" />


### Data Flow

1. **Extraction**: Azure Data Factory connects to GitHub repository via HTTP
2. **Raw Storage**: Data loaded as-is into ADLS Gen2 raw data container
3. **Transformation**: Databricks notebook reads raw data, applies Spark transformations
4. **Processed Storage**: Cleaned data written back to ADLS Gen2 transformed data container
5. **Analytics**: Synapse Analytics queries transformed data for insights
6. **Visualization**: Results consumed by BI tools for dashboard creation

---

## üõ†Ô∏è Technology Stack

### Cloud Platform
- **Microsoft Azure** - Primary cloud infrastructure provider

### Core Services

| Service | Purpose | Version/SKU |
|---------|---------|-------------|
| **Azure Data Factory** | ETL/ELT orchestration and data pipeline management | Standard |
| **Azure Data Lake Storage Gen2** | Hierarchical data lake with blob storage capabilities | Standard LRS |
| **Azure Databricks** | Apache Spark-based analytics platform | Premium |
| **Azure Synapse Analytics** | Cloud data warehouse and analytics service | Standard |

### Programming & Frameworks

| Technology | Purpose |
|------------|---------|
| **Python 3.x** | Primary programming language for transformations |
| **PySpark** | Distributed data processing framework |
| **SQL** | Data querying and analysis |

### Data Formats
- **CSV** - Input/output file format

---

## Setting up the Environment

### Step 1: Configure Azure Storage

#### 1.1 Create Storage Account
```bash
# Navigate to Azure Portal ‚Üí Storage Accounts ‚Üí Create
```

**Configuration:**
- **Subscription**: Free Trial
- **Resource Group**: `tokyo-olympic-rg` (create new)
- **Storage Account Name**: `tokyoolympicdata` (must be globally unique)
- **Region**: Select nearest region (e.g., `Southeast Asia`)
- **Performance**: Standard
- **Redundancy**: LRS (Locally Redundant Storage)

**Advanced Settings:**
- ‚úÖ Enable hierarchical namespace (critical for Data Lake Gen2)

#### 1.2 Create Container and Folder Structure
```
Container: tokyo-olympic-data
‚îú‚îÄ‚îÄ raw-data/           # Stores original ingested data
‚îî‚îÄ‚îÄ transform-data/     # Stores processed data
```

**Steps:**
1. Navigate to Storage Account ‚Üí Containers
2. Click "+ Container"
3. Name: `tokyo-olympic-data`
4. Create folders: `raw-data` and `transform-data`

---

### Step 2: Setup Azure Data Factory

#### 2.1 Create Data Factory Instance
```bash
# Azure Portal ‚Üí Data Factory ‚Üí Create
```

**Configuration:**
- **Resource Group**: `tokyo-olympic-rg`
- **Name**: `tokyo-olympic-df`
- **Region**: Same as storage account
- **Version**: V2

#### 2.2 Launch Data Factory Studio

1. Click "Launch Studio" from Data Factory overview
2. Navigate to **Author** tab (pencil icon)

---

### Step 3: Configure Azure Databricks

#### 3.1 Create Databricks Workspace
```bash
# Azure Portal ‚Üí Azure Databricks ‚Üí Create
```

**Configuration:**
- **Resource Group**: `tokyo-olympic-rg`
- **Workspace Name**: `tokyo-olympic-db`
- **Region**: Same as previous resources
- **Pricing Tier**: Premium (for full features)

#### 3.2 Create Compute Cluster

1. Launch Databricks workspace
2. Navigate to **Compute** ‚Üí Create Compute
3. **Configuration**:
   - **Cluster Mode**: Single Node
   - **Databricks Runtime**: 11.3 LTS (or latest LTS)
   - **Node Type**: Standard_DS3_v2 (4 cores, 14GB RAM)
   - **Terminate after**: 30 minutes of inactivity

---

### Step 4: Setup App Registration (for Databricks-ADLS Authentication)

#### 4.1 Register Application
```bash
# Azure Portal ‚Üí App Registrations ‚Üí New Registration
```

**Configuration:**
- **Name**: `databricks-app-01`
- **Supported Account Types**: Single tenant
- **Redirect URI**: Leave blank

**Collect Credentials** (save these securely):
```
Application (client) ID: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx
Directory (tenant) ID: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx
```

#### 4.2 Create Client Secret

1. Navigate to **Certificates & Secrets**
2. Click **New Client Secret**
3. **Description**: `secret-key`
4. **Expires**: 24 months
5. **Copy the Value** (not Secret ID) - save immediately, it won't be shown again
```
Secret Value: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
```

#### 4.3 Assign Storage Permissions
```bash
# Storage Account ‚Üí Access Control (IAM) ‚Üí Add Role Assignment
```

- **Role**: Storage Blob Data Contributor
- **Assign Access To**: User, group, or service principal
- **Members**: Select `databricks-app-01`
- **Review + Assign**

---

### Step 5: Environment Variables Setup

Create a configuration file (for reference, not executed):
```python
# databricks_config.py
configs = {
    "fs.azure.account.auth.type": "OAuth",
    "fs.azure.account.oauth.provider.type": 
        "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider",
    "fs.azure.account.oauth2.client.id": "<APPLICATION_CLIENT_ID>",
    "fs.azure.account.oauth2.client.secret": "<CLIENT_SECRET_VALUE>",
    "fs.azure.account.oauth2.client.endpoint": 
        "https://login.microsoftonline.com/<TENANT_ID>/oauth2/token"
}

# Mount Configuration
STORAGE_ACCOUNT_NAME = "tokyoolympicdata"
CONTAINER_NAME = "tokyo-olympic-data"
MOUNT_POINT = "/mnt/tokyoolympic"
```

---

## üìò Implementation Guide

### Phase 1: Data Ingestion Pipeline

#### 1.1 Create Linked Services

**For Source (GitHub HTTP)**:

1. In Data Factory Studio ‚Üí Manage ‚Üí Linked Services ‚Üí New
2. Select **HTTP** connector
3. Configuration:
```
   Name: athletes_http
   Base URL: https://raw.githubusercontent.com/<your-repo>/athletes.csv
   Authentication Type: Anonymous
```

**For Sink (Azure Data Lake Gen2)**:

1. New Linked Service ‚Üí Azure Data Lake Storage Gen2
2. Configuration:
```
   Name: adls_gen2
   Authentication: Account Key
   Storage Account: tokyoolympicdata
```

#### 1.2 Build Copy Pipeline

Create pipeline for each dataset:
```
Pipeline: data-ingestion
‚îú‚îÄ‚îÄ Copy_Athletes
‚îú‚îÄ‚îÄ Copy_Coaches
‚îú‚îÄ‚îÄ Copy_EntriesGender
‚îú‚îÄ‚îÄ Copy_Medals
‚îî‚îÄ‚îÄ Copy_Teams
```

**Copy Activity Configuration** (example for Athletes):
- **Source**:
  - Dataset: CSV from HTTP (athletes.csv)
  - First row as header: ‚úÖ
- **Sink**:
  - Dataset: ADLS Gen2 CSV
  - File path: `tokyo-olympic-data/raw-data/athletes.csv`
  - Copy method: Sequential

**Pipeline Execution**:
1. Click **Debug** to test
2. Monitor execution in **Monitor** tab
3. Verify files in Storage Account

---

### Phase 2: Data Transformation (Databricks)

#### 2.1 Mount ADLS to Databricks

Create new notebook: `tokyo-olympic-transformation`
```python
# Cell 1: Import required libraries
from pyspark.sql.functions import col
from pyspark.sql.types import IntegerType, StringType

# Cell 2: Configure ADLS Gen2 connection
configs = {
    "fs.azure.account.auth.type": "OAuth",
    "fs.azure.account.oauth.provider.type": 
        "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider",
    "fs.azure.account.oauth2.client.id": "<YOUR_CLIENT_ID>",
    "fs.azure.account.oauth2.client.secret": "<YOUR_CLIENT_SECRET>",
    "fs.azure.account.oauth2.client.endpoint": 
        "https://login.microsoftonline.com/<YOUR_TENANT_ID>/oauth2/token"
}

# Cell 3: Mount ADLS container
dbutils.fs.mount(
    source = "abfss://tokyo-olympic-data@tokyoolympicdata.dfs.core.windows.net/",
    mount_point = "/mnt/tokyoolympic",
    extra_configs = configs
)

# Cell 4: Verify mount
display(dbutils.fs.ls("/mnt/tokyoolympic/raw-data"))
```

#### 2.2 Read Raw Data
```python
# Read all datasets
athletes = spark.read.format("csv") \
    .option("header", "true") \
    .load("/mnt/tokyoolympic/raw-data/athletes.csv")

coaches = spark.read.format("csv") \
    .option("header", "true") \
    .load("/mnt/tokyoolympic/raw-data/coaches.csv")

entries_gender = spark.read.format("csv") \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .load("/mnt/tokyoolympic/raw-data/entriesgender.csv")

medals = spark.read.format("csv") \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .load("/mnt/tokyoolympic/raw-data/medals.csv")

teams = spark.read.format("csv") \
    .option("header", "true") \
    .load("/mnt/tokyoolympic/raw-data/teams.csv")
```

#### 2.3 Apply Transformations

**Schema Validation and Type Casting**:
```python
# Validate and display schemas
athletes.printSchema()
coaches.printSchema()

# Transform entries_gender: Cast numeric columns
entries_gender = entries_gender \
    .withColumn("Female", col("Female").cast(IntegerType())) \
    .withColumn("Male", col("Male").cast(IntegerType())) \
    .withColumn("Total", col("Total").cast(IntegerType()))

# Verify transformation
entries_gender.printSchema()
entries_gender.show(5)
```

**Sample Analytics**:
```python
# Top countries by gold medals
top_gold_countries = medals \
    .select("Team_Country", "Gold") \
    .orderBy(col("Gold").desc())

top_gold_countries.show(10)

# Calculate average entries by gender
from pyspark.sql.functions import col

entries_with_avg = entries_gender \
    .withColumn("Avg_Female", col("Female") / col("Total")) \
    .withColumn("Avg_Male", col("Male") / col("Total"))

entries_with_avg.show(10)
```

#### 2.4 Write Transformed Data
```python
# Write to transformed data layer
athletes.repartition(1).write \
    .mode("overwrite") \
    .option("header", "true") \
    .csv("/mnt/tokyoolympic/transform-data/athletes")

coaches.repartition(1).write \
    .mode("overwrite") \
    .option("header", "true") \
    .csv("/mnt/tokyoolympic/transform-data/coaches")

entries_gender.repartition(1).write \
    .mode("overwrite") \
    .option("header", "true") \
    .csv("/mnt/tokyoolympic/transform-data/entriesgender")

medals.repartition(1).write \
    .mode("overwrite") \
    .option("header", "true") \
    .csv("/mnt/tokyoolympic/transform-data/medals")

teams.repartition(1).write \
    .mode("overwrite") \
    .option("header", "true") \
    .csv("/mnt/tokyoolympic/transform-data/teams")
```

---

### Phase 3: Analytics Setup (Synapse Analytics)

#### 3.1 Create Synapse Workspace
```bash
# Azure Portal ‚Üí Azure Synapse Analytics ‚Üí Create
```

**Configuration:**
- **Resource Group**: `tokyo-olympic-rg`
- **Workspace Name**: `tokyo-olympic-synapse`
- **Region**: Same as other resources
- **Data Lake Storage**: Select existing `tokyoolympicdata`

#### 3.2 Query Transformed Data

1. Launch Synapse Studio
2. Navigate to **Data** ‚Üí Linked ‚Üí Azure Data Lake Storage Gen2
3. Browse to `transform-data` folder
4. Right-click on dataset ‚Üí **New SQL Script** ‚Üí **Select TOP 100**

**Sample Analytical Queries**:
```sql
-- Query 1: Top 10 countries by total medals
SELECT 
    Team_Country,
    Gold,
    Silver,
    Bronze,
    Total,
    Rank_by_Total
FROM medals
ORDER BY Total DESC
LIMIT 10;

-- Query 2: Gender distribution by discipline
SELECT 
    Discipline,
    Female,
    Male,
    Total,
    ROUND((Female * 100.0 / Total), 2) AS Female_Percentage
FROM entriesgender
ORDER BY Total DESC;

-- Query 3: Athlete count by country
SELECT 
    Country,
    COUNT(*) AS Athlete_Count
FROM athletes
GROUP BY Country
ORDER BY Athlete_Count DESC;
```

---

## üìñ Usage

### Running the Complete Pipeline

#### 1. **Data Ingestion** (Azure Data Factory)
```bash
1. Open Azure Data Factory Studio
2. Navigate to Author ‚Üí Pipelines ‚Üí data-ingestion
3. Click "Debug" or "Add Trigger" ‚Üí "Trigger Now"
4. Monitor execution in Monitor tab
5. Verify files in ADLS raw-data folder
```

**Expected Output**: 5 CSV files in `raw-data/` folder

#### 2. **Data Transformation** (Databricks)
```bash
1. Open Databricks Workspace
2. Navigate to Workspace ‚Üí Notebooks
3. Open tokyo-olympic-transformation notebook
4. Attach cluster (start if stopped)
5. Run all cells (Cell ‚Üí Run All)
6. Verify output in ADLS transform-data folder
```

**Expected Output**: 5 processed datasets in `transform-data/` folder

#### 3. **Analytics** (Synapse Analytics)
```bash
1. Open Synapse Studio
2. Navigate to Develop ‚Üí SQL Scripts
3. Create new script or open existing
4. Connect to built-in serverless pool
5. Execute analytical queries
6. Export results or build visualizations
```

---

### Monitoring and Troubleshooting

#### Data Factory Pipeline Monitoring
```
Location: Data Factory Studio ‚Üí Monitor
Check:
- Pipeline run status
- Activity-level details
- Error messages and logs
- Duration and throughput
```

#### Databricks Job Monitoring
```
Location: Databricks ‚Üí Clusters ‚Üí Event Log
Check:
- Cluster startup time
- Cell execution duration
- Spark UI for job details
- Driver and executor logs
```

#### Common Issues and Solutions

| Issue | Cause | Solution |
|-------|-------|----------|
| **Mount fails in Databricks** | Incorrect credentials or permissions | Verify App Registration credentials; Check Storage IAM roles |
| **Pipeline copy activity fails** | Network/firewall issues | Check NSG rules; Verify linked service connections |
| **Schema mismatch errors** | Data type inconsistencies | Use `inferSchema` option; Explicit type casting |
| **Out of memory errors** | Insufficient cluster resources | Increase cluster size; Optimize partitioning |
| **Access denied errors** | Missing RBAC permissions | Grant appropriate roles in IAM |

---

## üìä Data Schema

### Athletes Dataset
```
PersonName: String - Name of the athlete
Country: String - Country representation
Discipline: String - Sport discipline
```

### Coaches Dataset
```
Name: String - Coach name
Country: String - Country of coach
Discipline: String - Sport discipline
Event: String - Specific event
```

### Entries Gender Dataset
```
Discipline: String - Sport discipline
Female: Integer - Number of female participants
Male: Integer - Number of male participants
Total: Integer - Total participants
```

### Medals Dataset
```
Rank: Integer - Country ranking
Team_Country: String - Country name
Gold: Integer - Gold medals count
Silver: Integer - Silver medals count
Bronze: Integer - Bronze medals count
Total: Integer - Total medals
Rank_by_Total: Integer - Ranking by total medals
```

### Teams Dataset
```
TeamName: String - Name of the team
Discipline: String - Sport discipline
Country: String - Country representation
Event: String - Specific event
```

## ‚ö†Ô∏è Dependencies / Limitations

| Category                | Item                         | Details |
|-------------------------|------------------------------|---------|
| Technical Dependencies  | Azure Account Requirements   | - Active Azure subscription (Free trial provides $200 credit for 30 days)<br>- Valid payment method for account verification<br>- Access to Azure Portal |
| Technical Dependencies  | Service Quotas               | - Regional resource availability may vary<br>- Default quota limits on compute resources<br>- Storage account limits (500 TB default) |
| Technical Dependencies  | Network Requirements         | - Stable internet connection for cloud operations<br>- Access to GitHub API endpoints<br>- HTTPS connectivity for Azure services |
| Known Limitations       | Cost Considerations          | - Databricks compute clusters incur charges when running<br>- Data Lake storage costs scale with data volume<br>- Data transfer charges for egress traffic<br>- Synapse Analytics has per-query or provisioned capacity costs |
| Known Limitations       | Performance Constraints      | - Single-node Databricks cluster limits processing speed<br>- HTTP-based data ingestion may be slower than native integrations<br>- CSV format less efficient than columnar formats (Parquet/ORC) |
| Known Limitations       | Security Limitations         | - Credentials stored in code (not production-ready)<br>- No Azure Key Vault integration in base implementation<br>- Anonymous authentication for GitHub repository |
| Known Limitations       | Scalability Boundaries       | - Manual pipeline creation for each data source<br>- Limited error handling and retry logic<br>- No automated schema evolution handling |
| Known Limitations       | Data Quality                 | - Basic validation only (schema casting, null handling)<br>- No comprehensive data quality frameworks<br>- Limited data profiling capabilities |
| Regional Restrictions   | Regional Service Availability | - Some Azure services may not be available in all regions<br>- Compliance requirements may restrict data residency |

---

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
